# -*- coding: utf-8 -*-
"""preprocessing _and_AraBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p7FiIevC7IESboGFhpyvaJfNFfogRNRI
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p ~/.kaggle
!cp /content/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!pip install kagglehub

!kaggle datasets download -d sdaiancai/sada2022 -f "train.csv" -p "/content/drive/MyDrive/Arabic_Dialect_Classification/"

!unzip /content/drive/MyDrive/Arabic_Dialect_Classification/train.csv.zip -d /content/drive/MyDrive/Arabic_Dialect_Classification/

import numpy as np # linear algebra
#np.random.seed(40)
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import plotly.express as px # data visualization
import re # preprocessing text
from nltk.corpus import stopwords # get stops words
from sklearn.metrics import confusion_matrix  # Compute confusion matrix to evaluate the accuracy of a classification.
from plotly.subplots import make_subplots  # plot sub plots
import plotly.graph_objects as go # data visualization
import matplotlib.pyplot as plt # data visualization

from tensorflow.keras.utils import to_categorical # Converts a class vector (integers) to binary class matrix.
from sklearn.preprocessing import LabelEncoder # Encode target labels with value between 0 and n_classes-1.
from collections import Counter # collection where elements are stored as dictionary keys and their counts are stored as dictionary values
import tensorflow as tf # deep learning library
from tensorflow.keras.preprocessing.text import Tokenizer # to convert text into tokens
from tensorflow.keras.preprocessing.sequence import pad_sequences  # to add padding into tokens sequances

#########
from sklearn.model_selection import train_test_split # to split data into train and test
# machine learning algorithms
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import RocCurveDisplay

#########
import warnings
warnings.filterwarnings('ignore')

import os

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

color = ['#3c8283']

# load dataset
df = pd.read_csv('/content/drive/MyDrive/Arabic_Dialect_Classification/train.csv')

df.shape

# show the head of the dataset
df.head(3)

# drop unused columns
df.drop(['Unnamed: 0','FileName','FullFileLength','SegmentStart','SegmentEnd'], axis= 1, inplace=True)
df.head(3)

df.isna().sum()

print("Cound of duplicated rows in the dataset: ",df.duplicated().sum())

# drop rows when Ground Truth Text is null
df.drop(df[df.GroundTruthText.isna()].index, inplace=True)

import nltk
nltk.download('stopwords')

df.head(3)

df.describe(include='object').T

df.describe(include='number').T

"""SegmentLength represents the length of a segment (likely in seconds).

Most segments are short:

50% are under ~3.2 seconds

75% are under ~7.5 seconds

The maximum is very large (161.31), meaning there are some very long outliers.

The mean (6.22) is higher than the median (3.21) → suggests a right-skewed distribution due to long segments.

This could affect modeling or padding if using audio or text sequences of different lengths.
"""

# Counts of samples for each Dialect
speaker_dialect_counts = df['SpeakerDialect'].value_counts()
fig = px.bar(x = speaker_dialect_counts.index,
             y = speaker_dialect_counts.values,
             title ="Counts of samples for each Dialect ",
             labels = {'x':'Speaker Dialect', 'y':'Counts of samples in the dataset'},
             color_discrete_sequence = color * len(speaker_dialect_counts),
             template = "simple_white",
             text=['{}'.format(p) for p in speaker_dialect_counts.values]
            )
fig.show()

"""Custom Preprocessing Implementation sourced from social media—we implemented two custom Python classes: PreprocessTweets and TweetsTokenizing. These classes provide a structured and layered approach to cleaning, normalizing, and preparing the text before feeding it into classification models."""

class PreprocessTweets:
    def __init__ (self, text):
        self.text = text

    def normalize_letters(self):
        self.text = re.sub("[إأآا]", "ا", self.text)
        self.text = re.sub("ى", "ي", self.text)
        self.text = re.sub("ؤ", "ء", self.text)
        self.text = re.sub("ئ", "ء", self.text)
        self.text = re.sub("ة", "ه", self.text)
        self.text = re.sub("گ", "ك", self.text)
        self.text =  re.sub(r'(.)\1+', r'\1', self.text)

    def remove_tashkeel(self):
        tashkeel = re.compile("""
                                  ّ    | # Tashdid
                                  َ    | # Fatha
                                  ً    | # Tanwin Fath
                                  ُ    | # Damma
                                  ٌ    | # Tanwin Damm
                                  ِ    | # Kasra
                                  ٍ    | # Tanwin Kasr
                                  ْ    | # Sukun
                                ـ     # Tatwil/Kashida
                                    """, re.VERBOSE)
        self.text = re.sub(tashkeel, '', self.text)

    def sub_chars(self):
        chlist = [chr(i) for i in range(1569, 1611)] + [chr(i) for i in range(1646, 1749)]
        self.text = ''.join([i if i in chlist else ' ' for i in self.text])
        self.text = ' '.join([u'{}'.format(i) for i in self.text.split() if len(i) > 1])
        self.text = re.sub(r'\d+', ' ', self.text)

    def remove_symbols_spaces_and_english(self):
        symbols1 = re.compile('[/(){}\[\]\|,;]')
        symbols2 = re.compile("[@|(|)|\||:|>|<|_|#|\.|+|÷|×|'|!|\?|٪|؟\|&|;|\*|[|]|{|}|-|،|_|’|;|!|:|^|&|%|/]")
        arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ'''

        emojies = re.compile("["
                            u"\U0001F600-\U0001F64F"
                            u"\U0001F300-\U0001F5FF"
                            u"\U0001F680-\U0001F6FF"
                            u"\U0001F1E0-\U0001F1FF"
                            u"\U00002500-\U00002BEF"
                            u"\U00002702-\U000027B0"
                            u"\U00002702-\U000027B0"
                            u"\U000024C2-\U0001F251"
                            u"\U0001f926-\U0001f937"
                            u"\U00010000-\U0010ffff"
                            u"\u2640-\u2642"
                            u"\u0640"
                            u"\u2600-\u2B55"
                            u"\u200d"
                            u"\u23cf"
                            u"\u23e9"
                            u"\u231a"
                            u"\ufe0f"  # dingbats
                            u"\u3030"
                            "]+", re.UNICODE)

        self.text = re.sub(emojies, ' ', self.text)
        self.text =  re.sub(symbols1, ' ', self.text)
        self.text =  re.sub(symbols2, ' ', self.text)

        translator = str.maketrans('', '', arabic_punctuations)
        self.text = self.text.translate(translator)

        self.text = self.text.replace('"', " ")
        self.text = self.text.replace('…', " ")
        self.text =  re.sub(r'\s*[A-Za-z]+\b', ' ' , self.text)

        while '  ' in self.text:
            self.text = self.text.replace('  ', ' ')

    def preprocessing_pipeline(self):
        self.normalize_letters()
        self.remove_tashkeel()
        self.sub_chars()
        self.remove_symbols_spaces_and_english()

        return self.text

class TweetsTokenizing:

    def __init__(self, text):
        self.text = text
        self.tokens = []

    def tokenize_text(self):
        tokens = word_tokenize(self.text)
        self.tokens = [token.strip('') for token in tokens]

    def removeStopWords(self):
        stopwords_list = stopwords.words('arabic')
        listStopWords = stopwords_list
        self.tokens = [i for i in self.tokens if not i in listStopWords]

    def remove_repeated_characters(self):
        repeat_pattern = re.compile(r'(\w*)(\w)\2(\w*)')
        match_substitution = r'\1\2\3'

        def replace(old_word):
            if wordnet.synsets(old_word):
                return old_word
            new_word = repeat_pattern.sub(match_substitution, old_word)
            return replace(new_word) if new_word != old_word else new_word

        self.tokens = [replace(word) for word in self.tokens]

    def tokenize_pipeline(self):
        self.tokenize_text()
        self.removeStopWords()
        self.remove_repeated_characters()

        return self.tokens

df["Cleanedtext"] = df["GroundTruthText"].apply(lambda text : PreprocessTweets(str(text)).preprocessing_pipeline())

"""This code applies a preprocessing function to every entry in the GroundTruthText column and stores the cleaned version in a new column Cleanedtext."""

from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer
import numpy as np
from sklearn import preprocessing

nltk.download('all')
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer

"""NLTK stands for Natural Language Toolkit — it's a powerful open-source Python library for working with human language data (text). It’s one of the oldest and most widely used libraries for Natural Language Processing (NLP) tasks."""

df["Cleaned_text"] = df["Cleanedtext"].apply(lambda text : TweetsTokenizing(text).tokenize_pipeline())

"""This code takes already cleaned text and tokenizes it, storing the result in a new DataFrame column.

So, now your DataFrame (df) has:

GroundTruthText: original text

Cleanedtext: cleaned version of the original

Cleaned_text: tokenized version of the cleaned text
"""

df[['GroundTruthText','Cleaned_text']].head(20)

words = [word for tokens in df["Cleaned_text"] for word in tokens]
sentence_lens = [len(tokens) for tokens in df["Cleaned_text"]]

VOC = sorted(list(set(words)))

print("%s words total, with a vocabulary size of %s" % (len(words), len(VOC)))
print("Max sentence length is %s" % max(sentence_lens))

from collections import Counter
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
counter = Counter(words)
counter.most_common(20)

"""used the Counter class from Pythons collections module to find the most common words"""

import pandas as pd

# Assume 'df' is your preprocessed DataFrame containing 'text' and 'label' columns
# If you're working with Hugging Face Dataset, you can convert it to a pandas DataFrame
df_cleaned = pd.DataFrame({
    'Cleaned_text': df['Cleanedtext'],  # This is the cleaned text column
    'SpeakerDialect': df['SpeakerDialect']  # This is the label column
})

# Save the cleaned dataset to a specific location in your Google Drive
file_path = '/content/drive/MyDrive/Arabic_Dialect_Classification/cleaned_dataset.csv'  # Path to save the file in your Google Drive

# Save the cleaned DataFrame as a CSV file
df_cleaned.to_csv(file_path, encoding="utf-8-sig", index=False)
# Print confirmation
print(f"Data saved to {file_path}")

"""# Installing the necessary packages"""

# Install the necessary packages for Arabic NLP processing
!pip install arabert  # Arabic BERT Preprocessing
!pip install transformers  # Hugging Face's Transformer Library for NLP
!pip install farasapy  # Farasa Library for Arabic NLP tasks
!pip install pyarabic  # Python library for Arabic text processing

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/Arabic_Dialect_Classification/cleaned_dataset.csv", engine="python")

data

data["SpeakerDialect"].unique()

# Mapping dialects to numbers
map_label = {
    'Najdi': 0,
    'More than 1 speaker اكثر من متحدث': 1,
    'Unknown': 2,
    'Khaliji': 3,
    'Hijazi': 4,
    'ModernStandardArabic': 5,
    'Notapplicable': 6,
    'Egyptian': 7,
    'Levantine': 8,
    'Yemeni': 9,
    'Maghrebi': 10,
    'Janubi': 11,
    'Shamali': 12,
    'Iraqi': 13
}

# Mapping numbers back to dialects
label_map = {v: k for k, v in map_label.items()}

#print(map_label)
#print(label_map)

data

"""# Preprocess Arabic Text using AraBERT"""

from arabert.preprocess import ArabertPreprocessor  # Import AraBERT Preprocessor

model_name = "bert-base-arabert"  # Choose AraBERT model
arabert_prep = ArabertPreprocessor(model_name=model_name)  # Initialize preprocessor

from sklearn.model_selection import train_test_split
# Rename columns if necessary
data = data.rename(columns={'SpeakerDialect': 'dialect', 'Cleaned_text': 'text'})

data

# Split into train (80%) and test (20%)
dataset, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['dialect'])

# Display dataset shapes
print("Train set size:", dataset.shape)
print("Test set size:", test_data.shape)

"""Apply AraBERT Preprocessing to Text

"""

dataset["text"]=dataset["text"].apply(lambda x:arabert_prep.preprocess(x))

test_data["text"]=test_data["text"].apply(lambda x:arabert_prep.preprocess(x))

dataset

# Define the path where you want to save the datasets
train_data_path = '/content/drive/MyDrive/Arabic_Dialect_Classification/dataset.csv'
test_data_path = '/content/drive/MyDrive/Arabic_Dialect_Classification/test_data.csv'

# Save the datasets to Google Drive
dataset.to_csv(train_data_path, index=False)
test_data.to_csv(test_data_path, index=False)

print(f"Datasets saved to:\n{train_data_path}\n{test_data_path}")

"""# Importing the necessary packages"""

from arabert.preprocess import ArabertPreprocessor
from sklearn.metrics import (accuracy_score, f1_score,recall_score)
from torch.utils.data import  Dataset
from transformers import (AutoConfig, AutoModelForSequenceClassification,
                        AutoTokenizer, BertTokenizer, Trainer,
                        TrainingArguments)
from transformers.data.processors.utils import InputFeatures

#chose bert model
model_name = 'aubmindlab/bert-base-arabert'
#asafaya/bert-base-arabic
#UBC-NLP/ARBERT
#UBC-NLP/MARBERT
#bert-base-multilingual-uncased
num_labels = 14
max_length = 120



"""## To work using PyTorch we need to create a classification dataset to load the data"""

class ClassificationDataset(Dataset):
    def __init__(self, text, target, model_name, max_len, label_map):
      super(ClassificationDataset).__init__()

      self.text = text
      self.target = target
      self.tokenizer_name = model_name
      self.tokenizer = AutoTokenizer.from_pretrained(model_name) # Load tokenizer
      self.max_len = max_len # Set maximum text length
      self.label_map = label_map


    def __len__(self):
      return len(self.text)

    def __getitem__(self,item):
      text = str(self.text[item])
      text = " ".join(text.split())

      inputs = self.tokenizer(
          text,
          max_length=self.max_len,
          padding='max_length',
          truncation=True
        )
      return InputFeatures(**inputs,label= self.target[item])

dataset['dialect'] = dataset['dialect'].map(map_label)
test_data['dialect'] = test_data['dialect'].map(map_label)

test_data=test_data[test_data['dialect'].isnull()==False]
dataset=dataset[dataset['dialect'].isnull()==False]

# Convert labels to integers
dataset['dialect'] = dataset['dialect'].astype(int)
test_data['dialect'] = test_data['dialect'].astype(int)

dataset.info()

"""## Creating datasets
This part of the code initializes PyTorch datasets for training and testing using the ClassificationDataset class.
"""

train_dataset = ClassificationDataset(
    dataset['text'].to_list(),   # Convert text column to a list (training data)
    dataset['dialect'].to_list(),  # Convert dialect labels to a list (training labels)
    model_name,  # Name of the pretrained model (e.g., 'bert-base-arabert')
    max_length,  # Maximum token length for input sequences
    map_label  # Dictionary mapping dialect names to numerical labels
)
test_dataset = ClassificationDataset(
    test_data['text'].to_list(),   # Convert text column to a list (test data)
    test_data['dialect'].to_list(),  # Convert dialect labels to a list (test labels)
    model_name,  # Name of the pretrained model
    max_length,  # Maximum sequence length
    map_label  # Label mapping dictionary
)

(len(train_dataset))

"""## Create a function that return a pretrained model ready to do classification"""

def model_init():
    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=num_labels)

"""## Metrics"""

def compute_metrics(p): #p should be of type EvalPrediction
  preds = np.argmax(p.predictions, axis=1)
  assert len(preds) == len(p.label_ids)
  macro_f1 = f1_score(p.label_ids,preds,average='macro')
  macro_recall = recall_score(p.label_ids,preds,average='macro')
  acc = accuracy_score(p.label_ids,preds)
  return {
      'macro_f1' : macro_f1,
      'accuracy': acc,
      'recall':macro_recall
  }

"""## Training arguments"""

from transformers import TrainingArguments, EarlyStoppingCallback

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Arabic_Dialect_Classification/train",
    adam_epsilon=1e-8,
    learning_rate=5e-6,
    fp16=True,  # استخدام حساب النقطة العائمة 16 بت إذا كان لديك V100/T4
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=4,
    num_train_epochs=7,  # تقليل عدد العصور لتجنب Overfitting
    warmup_ratio=0.1,  # استخدام Warmup لتحسين الاستقرار
    do_eval=True,
    weight_decay=0.01 ,
    eval_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=5,  # تقليل عدد النماذج المحفوظة لتوفير المساحة
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss',
    greater_is_better=False,
    report_to=[],
)

"""## Creating the trainer"""

trainer = Trainer(
    model = model_init(),
    args = training_args,
    train_dataset = train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)

"""# Tarining"""

trainer.train()

"""## Saving the model"""

import os
model_save_path = '/content/drive/MyDrive/Arabic_Dialect_Classification/model_checkpoint'
# Set the model configuration for label mappings
trainer.model.config.label2id = map_label
trainer.model.config.id2label = label_map
trainer.save_model(os.path.join(model_save_path, 'checkpoint-1'))
train_dataset.tokenizer.save_pretrained(os.path.join(model_save_path, 'checkpoint-1'))

# Define the path in Google Drive
model_save_path = '/content/drive/MyDrive/Arabic_Dialect_Classification/model_checkpoint'

# Set the model configuration for label mappings
trainer.model.config.label2id = map_label
trainer.model.config.id2label = label_map

# Save the model to Google Drive
trainer.save_model(model_save_path)

# Save the tokenizer to Google Drive
train_dataset.tokenizer.save_pretrained(model_save_path)