# -*- coding: utf-8 -*-
"""LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y3FGFsEuDtKJJIevjcFfLdX0qSWZ4wnw

# **Objective**:

- Use TF-IDF with different feature limits to compare performance.
- Train and evaluate a Logistic Regression model
- Handle class imbalance using class weights.

##Import Necessary Libraries
"""

import torch

# If there's a GPU available...
if torch.cuda.is_available():

    # Tell PyTorch to use the GPU.
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

pip install livelossplot

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, accuracy_score, classification_report
from sklearn.naive_bayes import ComplementNB
from sklearn.pipeline import Pipeline
import joblib

!pip install --upgrade tensorflow

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense
from keras.callbacks import EarlyStopping
from keras.utils import to_categorical
from keras import Model, Input
from keras.layers import LSTM, Embedding, Dense
from keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional
from keras.callbacks import ModelCheckpoint, EarlyStopping
from livelossplot.tf_keras import PlotLossesCallback

"""##Load Data"""

from google.colab import drive
drive.mount('/content/drive')

df_cleaned = pd.read_csv("/content/drive/MyDrive/Arabic_Dialect_Classification/cleaned_dataset.csv", engine="python")

df_cleaned



pd.set_option('display.max_rows',500)
pd.set_option('display.max_colwidth',500)

df_cleaned.isnull().values.any()

df_cleaned.isnull().sum()

"""##Data Splitting"""

x=df_cleaned.drop(['SpeakerDialect'], axis=1)
y=df_cleaned['SpeakerDialect']

print("shape of x :",x.shape)
print("shape of y  :" ,y.shape)

x_train ,x_test ,y_train , y_test = train_test_split(x,y,test_size=0.15, random_state=24, shuffle=True,stratify=y)

x_train, x_val , y_train, y_val = train_test_split(x_train, y_train, test_size=0.10, random_state=42,stratify=y_train)

x_train.isnull().sum()

"""# Tokenizer and Model

## ML Models

We extract all **unique words** from the `Cleaned_text` column of the training set using a Python `set`. This helps us:

- Get an understanding of the vocabulary size.
- Prepare for building the tokenizer or embedding layer.

Steps:
1. Iterate over each cleaned text sample.
2. Tokenize by splitting on whitespace.
3. Add tokens to a set, which automatically filters duplicates.

The final count gives us the **number of distinct words** present in the training data.
"""

# Use a Set to Store Unique Words
unique_words = set()

for text in x_train['Cleaned_text']:
    words = text.split()  # Tokenize
    unique_words.update(words)

# Number of Unique Words
num_unique_words = len(unique_words)
print(f'Number of unique words: {num_unique_words}')

"""**Term Frequency-Inverse Document Frequency (TF-IDF):**
-  TF-IDF assigns weights to terms based on their frequency in individual documents and their inverse frequency across all documents. This technique helps capture the importance of terms in distinguishing between documents.
"""

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000)
x_train_tfidf = vectorizer.fit_transform(x_train['Cleaned_text'])
x_val_tfidf = vectorizer.transform(x_val['Cleaned_text'])
x_test_tfidf = vectorizer.transform(x_test['Cleaned_text'])

# Initialize LabelEncoder
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Check the mapping
print("Label Mapping:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

"""##Logistic regression"""

# Train a Logistic Regression model
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(x_train_tfidf, y_train_encoded)

# Evaluate on validation set
y_val_pred = model.predict(x_val_tfidf)
val_accuracy = accuracy_score(y_val_encoded, y_val_pred)
print(f'Validation Accuracy: {val_accuracy:.4f}')

# Evaluate on test set
y_test_pred = model.predict(x_test_tfidf)
test_accuracy = accuracy_score(y_test_encoded, y_test_pred)
print(f'Test Accuracy: {test_accuracy:.4f}')

from sklearn.metrics import classification_report

# Get full label range
labels = list(range(len(label_encoder.classes_)))
target_names = label_encoder.classes_

# Print classification report
print(classification_report(
    y_test_encoded,
    y_test_pred,
    labels=labels,
    target_names=target_names,
    zero_division=0  # Avoids division-by-zero warnings
))

"""### use 10000  Unique Words

"""

# TF-IDF Vectorization
vectorizer_ = TfidfVectorizer(max_features=10000)
x_train_tfidf = vectorizer_.fit_transform(x_train['Cleaned_text'])
x_val_tfidf = vectorizer_.transform(x_val['Cleaned_text'])
x_test_tfidf = vectorizer_.transform(x_test['Cleaned_text'])

# Train a Logistic Regression model
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(x_train_tfidf, y_train_encoded)

# Evaluate on validation set
y_val_pred = model.predict(x_val_tfidf)
val_accuracy = accuracy_score(y_val_encoded, y_val_pred)
val_Precision = precision_score(y_val_encoded, y_val_pred, average='macro')
print(f'Validation Accuracy: {val_accuracy:.4f}')
print(f'Precision: {val_Precision:.4f}')

# Evaluate on test set
y_test_pred = model.predict(x_test_tfidf)
test_accuracy = accuracy_score(y_test_encoded, y_test_pred)
print(f'Test Accuracy: {test_accuracy:.4f}')

from sklearn.metrics import classification_report

# Get full label range
labels = list(range(len(label_encoder.classes_)))
target_names = label_encoder.classes_

# Print classification report
print(classification_report(
    y_test_encoded,
    y_test_pred,
    labels=labels,
    target_names=target_names,
    zero_division=0  # Avoids division-by-zero warnings
))

"""Use All Unique Words"""

# TF-IDF Vectorization
vectorizer_ = TfidfVectorizer(max_features=num_unique_words)
x_train_tfidf = vectorizer_.fit_transform(x_train['Cleaned_text'])
x_val_tfidf = vectorizer_.transform(x_val['Cleaned_text'])
x_test_tfidf = vectorizer_.transform(x_test['Cleaned_text'])

# Train a Logistic Regression model
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(x_train_tfidf, y_train_encoded)

# Evaluate on validation set
y_val_pred = model.predict(x_val_tfidf)
val_accuracy = accuracy_score(y_val_encoded, y_val_pred)
val_Precision = precision_score(y_val_encoded, y_val_pred, average='macro')
print(f'Validation Accuracy: {val_accuracy:.4f}')
print(f'Precision: {val_Precision:.4f}')

# Evaluate on test set
y_test_pred = model.predict(x_test_tfidf)
test_accuracy = accuracy_score(y_test_encoded, y_test_pred)
print(f'Test Accuracy: {test_accuracy:.4f}')

from sklearn.metrics import classification_report

# Get full label range
labels = list(range(len(label_encoder.classes_)))
target_names = label_encoder.classes_

# Print classification report
print(classification_report(
    y_test_encoded,
    y_test_pred,
    labels=labels,
    target_names=target_names,
    zero_division=0  # Avoids division-by-zero warnings
))

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Get unique classes
classes = np.unique(y_train_encoded)

# Compute weights
weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_encoded)

# Make dict: {class_index: weight}
class_weight_dict = dict(zip(classes, weights))

# Train model
model = LogisticRegression(class_weight=class_weight_dict, max_iter=1000, random_state=42)
model.fit(x_train_tfidf, y_train_encoded)

# Evaluate on test set
y_test_pred = model.predict(x_test_tfidf)
test_accuracy = accuracy_score(y_test_encoded, y_test_pred)
print(f'Test Accuracy: {test_accuracy:.4f}')

from sklearn.metrics import classification_report

# Get full label range
labels = list(range(len(label_encoder.classes_)))
target_names = label_encoder.classes_

# Print classification report
print(classification_report(
    y_test_encoded,
    y_test_pred,
    labels=labels,
    target_names=target_names,
    zero_division=0  # Avoids division-by-zero warnings
))